---
title: 量子機器學習程式庫
description: 瞭解如何在量子系統上使用機器學習服務
author: alexeib2
ms.author: alexeib
ms.date: 11/22/2019
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.intro
no-loc:
- Q#
- $$v
ms.openlocfilehash: 9f7f892fb2b76432942c86163497c22f0c73d51f
ms.sourcegitcommit: 9b0d1ffc8752334bd6145457a826505cc31fa27a
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 09/21/2020
ms.locfileid: "90833798"
---
# <a name="introduction-to-quantum-machine-learning"></a><span data-ttu-id="19311-103">量子 Machine Learning 簡介</span><span class="sxs-lookup"><span data-stu-id="19311-103">Introduction to Quantum Machine Learning</span></span>

## <a name="framework-and-goals"></a><span data-ttu-id="19311-104">架構與目標</span><span class="sxs-lookup"><span data-stu-id="19311-104">Framework and goals</span></span>

<span data-ttu-id="19311-105">量子編碼和資訊處理是傳統機器學習量子分類器的強大替代方案。</span><span class="sxs-lookup"><span data-stu-id="19311-105">Quantum encoding and processing of information is a powerful alternative to classical machine learning Quantum classifiers.</span></span> <span data-ttu-id="19311-106">尤其是，它可讓我們在量子暫存器中編碼相當於特徵數量的資料，並有系統地採用量子纏結做為運算資源，並採用類別推斷的量子測量。</span><span class="sxs-lookup"><span data-stu-id="19311-106">In particular, it allows us to encode data in quantum registers that are concise relative to the number of features, systematically employing quantum entanglement as computational resource and employing quantum measurement for class inference.</span></span>
<span data-ttu-id="19311-107">以電路為中心的量子分類器是相當簡單的量子解決方案，結合了資料編碼與快速的 entangling/抽絲剝繭量子電路，接著測量以推斷資料範例的類別標籤。</span><span class="sxs-lookup"><span data-stu-id="19311-107">Circuit centric quantum classifier is a relatively simple quantum solution that combines data encoding with a rapidly entangling/disentangling quantum circuit followed by measurement to infer class labels of data samples.</span></span>
<span data-ttu-id="19311-108">其目標是要確保主體線路的傳統特性和儲存，以及線路參數的混合式量子/傳統訓練，即使是非常大的功能空間也是如此。</span><span class="sxs-lookup"><span data-stu-id="19311-108">The goal is to ensure classical characterization and storage of subject circuits, as well as hybrid quantum/classical training of the circuit parameters even for extremely large feature spaces.</span></span>

## <a name="classifier-architecture"></a><span data-ttu-id="19311-109">分類器架構</span><span class="sxs-lookup"><span data-stu-id="19311-109">Classifier architecture</span></span>

<span data-ttu-id="19311-110">分類是受監督的機器學習工作，其目標是要推斷 \{ 特定資料範例的類別標籤 $ y_1、y_2、\ldots y_d \} $。</span><span class="sxs-lookup"><span data-stu-id="19311-110">Classification is a supervised machine learning task, where the goal is to infer class labels $\{y_1,y_2,\ldots,y_d\}$ of certain data samples.</span></span> <span data-ttu-id="19311-111">「訓練資料集」是範例 $ \mathcal{D} = \{ (x，y) } $ 的集合，其中包含已知預先指派的標籤。</span><span class="sxs-lookup"><span data-stu-id="19311-111">The "training data set" is a collection of samples $\mathcal{D}=\{(x,y)}$ with known pre-assigned labels.</span></span> <span data-ttu-id="19311-112">此處 $x $ 是資料範例，$y $ 是其稱為「定型標籤」的已知標籤。</span><span class="sxs-lookup"><span data-stu-id="19311-112">Here $x$ is a data sample and $y$ is its known label called "training label".</span></span>
<span data-ttu-id="19311-113">量子分類有點類似傳統的方法，包括三個步驟：</span><span class="sxs-lookup"><span data-stu-id="19311-113">Somewhat similar to traditional methods, quantum classification consists of three steps:</span></span>
- <span data-ttu-id="19311-114">資料編碼</span><span class="sxs-lookup"><span data-stu-id="19311-114">data encoding</span></span>
- <span data-ttu-id="19311-115">分類器狀態的準備工作</span><span class="sxs-lookup"><span data-stu-id="19311-115">preparation of a classifier state</span></span>
- <span data-ttu-id="19311-116">度量是因為測量的概率本質，這三個步驟必須重複多次。</span><span class="sxs-lookup"><span data-stu-id="19311-116">measurement Due to the probabilistic nature of the measurement, these three steps must be repeated multiple times.</span></span> <span data-ttu-id="19311-117">分類器狀態的編碼和計算都是透過 *量子電路*來完成。</span><span class="sxs-lookup"><span data-stu-id="19311-117">Both the encoding and the computing of the classifier state are done by means of *quantum circuits*.</span></span> <span data-ttu-id="19311-118">雖然編碼電路通常是資料驅動和無參數，但分類器電路包含一組足夠的 learnable 參數。</span><span class="sxs-lookup"><span data-stu-id="19311-118">While the encoding circuit is usually data-driven and parameter-free, the classifier circuit contains a sufficient set of learnable parameters.</span></span> 

<span data-ttu-id="19311-119">在建議的解決方案中，分類器電路是由單一量子位的旋轉和雙量子位控制的旋轉所組成。</span><span class="sxs-lookup"><span data-stu-id="19311-119">In the proposed solution the classifier circuit is composed of single-qubit rotations and two-qubit controlled rotations.</span></span> <span data-ttu-id="19311-120">這裡的 learnable 參數是旋轉角度。</span><span class="sxs-lookup"><span data-stu-id="19311-120">The learnable parameters here are the rotation angles.</span></span> <span data-ttu-id="19311-121">「旋轉」和「受控制的旋轉」閘道已知為量子計算的 *通用* ，這表示任何單一權數矩陣都可以分解成由這類閘道組成的夠長線路。</span><span class="sxs-lookup"><span data-stu-id="19311-121">The rotation and controlled rotation gates are known to be *universal* for quantum computation, which means that any unitary weight matrix can be decomposed into a long enough circuit consisting of such gates.</span></span>

<span data-ttu-id="19311-122">在建議的版本中，只支援一個電路，後面接著單一頻率的估計。</span><span class="sxs-lookup"><span data-stu-id="19311-122">In the proposed version, only one circuit followed by a single frequency estimation is supported.</span></span>
<span data-ttu-id="19311-123">因此，解決方案是具有低度多項式核心的支援向量機器的量子類比。</span><span class="sxs-lookup"><span data-stu-id="19311-123">Thus, the solution is a quantum analog of a support vector machine with a low-degree polynomial kernel.</span></span>

![多層認知與以電路為中心的分類器](~/media/DLvsQCC.png)

<span data-ttu-id="19311-125">簡單的量子分類器設計可以與傳統支援向量機器 (SVM) 解決方案。</span><span class="sxs-lookup"><span data-stu-id="19311-125">A simple quantum classifier design can be compared to a traditional support vector machine (SVM) solution.</span></span> <span data-ttu-id="19311-126">在 SVM 的情況下，資料 $x 範例的推斷是使用最佳的核心表單 $ \sum \ Alpha_j k (x_j x) $，其中 $k $ 是特定的核心函式來完成。</span><span class="sxs-lookup"><span data-stu-id="19311-126">The inference for a data sample $x$ in case of SVM is done using an optimal kernel form $\sum \alpha_j  k(x_j,x)$ where $k$ is a certain kernel function.</span></span>

<span data-ttu-id="19311-127">相反地，量子分類器會使用預測 $p (y │ x、U ( \theta) # B3 = 〈 U ( \theta) x |M |U ( \theta) x 〉 $，這在精神中很類似，但技術上相當不同。</span><span class="sxs-lookup"><span data-stu-id="19311-127">By contrast, a quantum classifier uses the predictor $p(y│x,U(\theta))=〈U(\theta)x|M|U(\theta)x〉$, which is similar in spirit but technically quite different.</span></span> <span data-ttu-id="19311-128">因此，使用簡單的調幅編碼時，$p (y │ x，U ( \theta) # B3 $ 是 amplitudes 中 $x $ 的二次形式，但此表單的係數不再獨立學習;它們會改為從電路的矩陣元素匯總 $U ( \theta) $，這通常會比向量 $x $ 的維度少很多的 learnable 參數 $ \theta $。</span><span class="sxs-lookup"><span data-stu-id="19311-128">Thus, when a straightforward amplitude encoding is used,  $p(y│x,U(\theta))$ is a quadratic form in the amplitudes of $x$, but the coefficients of this form are no longer learned independently; they are instead aggregated from the matrix elements of the circuit $U(\theta)$, which typically has significantly fewer learnable parameters $\theta$ than the dimension of the vector $x$.</span></span> <span data-ttu-id="19311-129">在原始功能中，$p (y │ x，U ( \theta) # B3 $ 的多項式程度可以增加至 $ 2 ^ l $，方法是在 $l $ 的 $x $ 複本上使用量子產品編碼。</span><span class="sxs-lookup"><span data-stu-id="19311-129">The polynomial degree of $p(y│x,U(\theta))$ in the original features can be increased to $2^l$ by using a quantum product encoding on $l$ copies of $x$.</span></span>

<span data-ttu-id="19311-130">我們的架構探討相當淺層的線路，因此必須 *快速 entangling* ，才能在所有範圍的資料功能之間抓取所有關聯性。</span><span class="sxs-lookup"><span data-stu-id="19311-130">Our architecture explores relatively shallow circuits, which therefore must be *rapidly entangling* in order to capture all the correlations between the data features at all ranges.</span></span> <span data-ttu-id="19311-131">下圖顯示最有用的快速 entangling 電路元件的範例。</span><span class="sxs-lookup"><span data-stu-id="19311-131">An example of the most useful rapidly entangling circuit component is shown on figure below.</span></span> <span data-ttu-id="19311-132">雖然具有此幾何的電路只包含 $3 n + 1 $ 閘道，但它所計算的單一權數矩陣可確保 $ 2 ^ n $ 功能之間有顯著的交叉交談。</span><span class="sxs-lookup"><span data-stu-id="19311-132">Even though a circuit with this geometry consists of only $3 n+1$ gates, the unitary weight matrix that it computes ensures significant cross-talk between $2^n$ features.</span></span>

![在5個量子位 (上快速 entangling 量子電路，) 有兩個迴圈層。](~/media/5-qubit-qccc.png)

<span data-ttu-id="19311-134">上述範例中的線路包含6個單一量子位閘道 $ (G_1、\ldots、G_5;G_ {16}) $ 和 10 2-量子位閘道 $ (G_6，\ldots，G_ {15}) $。</span><span class="sxs-lookup"><span data-stu-id="19311-134">The circuit in the above example consists of 6 single-qubit gates $(G_1,\ldots,G_5; G_{16})$ and 10 two-qubits gates $(G_6,\ldots,G_{15})$.</span></span> <span data-ttu-id="19311-135">假設每個閘道都定義了一個 learnable 參數，我們有16個 learnable 參數，而5量子位希伯特空間的維度是32。</span><span class="sxs-lookup"><span data-stu-id="19311-135">Assuming that each of the gates is defined with one learnable parameter we have 16 learnable parameters, while the dimension of the 5-qubit Hilbert space is 32.</span></span> <span data-ttu-id="19311-136">當 $n $ 為奇數時，可輕鬆將這類線路幾何一般化至任何 $n $ 量子位暫存器，並為 $ 2 ^ n $ 維度功能空間產生具有 $3 n + 1 $ 參數的線路。</span><span class="sxs-lookup"><span data-stu-id="19311-136">Such circuit geometry can be easily generalized to any $n$-qubit register, when $n$ is odd, yielding circuits with $3 n+1$ parameters for $2^n$-dimensional feature space.</span></span>

## <a name="classifier-training-as-a-supervised-learning-task"></a><span data-ttu-id="19311-137">分類定型作為受監督的學習工作</span><span class="sxs-lookup"><span data-stu-id="19311-137">Classifier training as a supervised learning task</span></span>

<span data-ttu-id="19311-138">分類器模型的定型牽涉到尋找其作業參數的最佳值，使其能將正確定型標籤在定型範例中推斷的平均可能性最大化。</span><span class="sxs-lookup"><span data-stu-id="19311-138">Training of a classifier model involves finding optimal values of its operational parameters, such that they maximize the average likelihood of inferring the correct training labels across the training samples.</span></span>
<span data-ttu-id="19311-139">在這裡，我們只關心兩個層級的分類，亦即 $d = $2 的案例，以及只有兩個標籤 $y _1 y_2 $ 的類別。</span><span class="sxs-lookup"><span data-stu-id="19311-139">Here, we concern ourselves with two level classification only, i.e. the case of $d=2$ and only two classes with the labels $y_1,y_2$.</span></span>

> [!NOTE]
> <span data-ttu-id="19311-140">將我們的方法一般化至任意數目類別的謹守原則一切方法，就是將量子位取代為 qudits，也就是以 $d $ 基礎狀態的量子單位，以及使用 $d $-向測量的雙向度量。</span><span class="sxs-lookup"><span data-stu-id="19311-140">A principled way of generalizing our methods to arbitrary number of classes is to replace qubits with qudits, i.e. quantum units with $d$ basis states, and the two-way measurement with $d$-way measurement.</span></span>

### <a name="likelihood-as-the-training-goal"></a><span data-ttu-id="19311-141">作為定型目標的可能性</span><span class="sxs-lookup"><span data-stu-id="19311-141">Likelihood as the training goal</span></span>

<span data-ttu-id="19311-142">給定 learnable 量子線路 $U ( \theta) $，其中 $ \theta $ 是參數的向量，並以 $M $ 表示最後的測量值，正確標籤推斷的平均可能性為 $ $ \begin{align} \mathcal{L} ( \theta) = \frac {1} {| \mathcal{D} |} \left ( \ sum_ { (x，y_1) \In\mathcal{d}} P (M = y_1 |U ( \theta) x) + \ sum_ { (x，y_2) \in\mathcal{D}} P (M = y_2 |U ( \theta) x) \right) \end{align} $ $，其中 $P (M = y | z) $ 是測量 $y $ in 量子狀態 $z $ 的機率。</span><span class="sxs-lookup"><span data-stu-id="19311-142">Given a learnable quantum circuit $U(\theta)$, where $\theta$ is a vector of parameters, and denoting the final measurement by $M$, the average likelihood of the correct label inference is $$ \begin{align} \mathcal{L}(\theta)=\frac{1}{|\mathcal{D}|} \left( \sum_{(x,y_1)\in\mathcal{D}} P(M=y_1|U(\theta) x) + \sum_{(x,y_2)\in\mathcal{D}} P(M=y_2|U(\theta) x)\right) \end{align} $$ where $P(M=y|z)$ is the probability of measuring $y$ in quantum state $z$.</span></span>
<span data-ttu-id="19311-143">在這裡，它會後綴以瞭解可能性函式 $ \mathcal{L} ( \theta) $ 在 $ \theta $ 中是平滑的，且在任何 $ \ theta_j $ 中的衍生都可由基本上與用來計算可能性函式本身的相同量子通訊協定計算。</span><span class="sxs-lookup"><span data-stu-id="19311-143">Here, it suffices to understand that the likelihood function $\mathcal{L}(\theta)$ is smooth in $\theta$ and its derivative in any $\theta_j$ can be computed by essentially the same quantum protocol as used for computing the likelihood function itself.</span></span> <span data-ttu-id="19311-144">這可讓您將 $ \mathcal{L} ( \theta) $ 依梯度下降優化。</span><span class="sxs-lookup"><span data-stu-id="19311-144">This allows for optimizing the $\mathcal{L}(\theta)$ by gradient descent.</span></span>

### <a name="classifier-bias-and-training-score"></a><span data-ttu-id="19311-145">分類偏差和定型分數</span><span class="sxs-lookup"><span data-stu-id="19311-145">Classifier bias and training score</span></span>

<span data-ttu-id="19311-146">有了 $ \theta $ 中參數的某些中繼 (或最終) 值時，我們需要識別單一真正的值，$b $ 知道要進行推斷的 *分類偏差* 。</span><span class="sxs-lookup"><span data-stu-id="19311-146">Given some intermediate (or final) values of the parameters in $\theta$, we need to identify a single real value $b$ know as *classifier bias* to do the inference.</span></span> <span data-ttu-id="19311-147">標籤推斷規則的運作方式如下：</span><span class="sxs-lookup"><span data-stu-id="19311-147">The label inference rule works as follows:</span></span> 
- <span data-ttu-id="19311-148">只有在 $P (M = y_2 時，才會將標籤 $y _2 $ 指派給範例 $x $U ( \theta) x) + b > $0.5 (RULE1)  (否則會指派標籤 $y _1 $) </span><span class="sxs-lookup"><span data-stu-id="19311-148">A sample $x$ is assigned label $y_2$ if and only if $P(M=y_2|U(\theta) x) + b > 0.5$  (RULE1) (otherwise it is assigned label $y_1$)</span></span>

<span data-ttu-id="19311-149">顯然 $b $ 必須在時間間隔 $ (-0.5、+ 0.5) $ 才有意義。</span><span class="sxs-lookup"><span data-stu-id="19311-149">Clearly $b$ must be in the interval $(-0.5,+0.5)$ to be meaningful.</span></span>

<span data-ttu-id="19311-150">如果根據 RULE1 推斷 $x $ 的標籤與 $y $ 不同，則會將定型案例 $ (x，y) \in \mathcal{D} $ 視為 $b *分類誤判* 。</span><span class="sxs-lookup"><span data-stu-id="19311-150">A training case $(x,y) \in \mathcal{D}$ is considered a *misclassification* given the bias $b$ if the label inferred for $x$ as per RULE1 is actually different from $y$.</span></span> <span data-ttu-id="19311-151">情形的整體數目是指定偏差 $b $ 的分類器 *定型分數* 。</span><span class="sxs-lookup"><span data-stu-id="19311-151">The overall number of misclassifications is the *training score* of the classifier given the bias $b$.</span></span> <span data-ttu-id="19311-152">*最佳*的分類偏差 $b $ 將定型分數降至最低。</span><span class="sxs-lookup"><span data-stu-id="19311-152">The *optimal* classifier bias $b$ minimizes the training score.</span></span> <span data-ttu-id="19311-153">由於預先電腦率估計 $ \{ P (M = y_2，因此很容易看出U ( \theta) x) | (x，\* ) \in\mathcal{D} \} $，二進位搜尋可以在間隔 $ (-0.5，+ 0.5) $ 中找到最佳的分類偏差，最多可達 $ \ log_2 (| \mathcal{D} |) $ 步驟。</span><span class="sxs-lookup"><span data-stu-id="19311-153">It is easy to see that, given the precomputed probability estimates $\{ P(M=y_2|U(\theta) x) | (x,\*)\in\mathcal{D} \}$, the optimal classifier bias can be found by binary search in interval $(-0.5,+0.5)$ by making at most $\log_2(|\mathcal{D}|)$ steps.</span></span>

### <a name="reference"></a><span data-ttu-id="19311-154">參考</span><span class="sxs-lookup"><span data-stu-id="19311-154">Reference</span></span>

<span data-ttu-id="19311-155">這種資訊應該足以開始使用程式碼。</span><span class="sxs-lookup"><span data-stu-id="19311-155">This information should be enough to start playing with the code.</span></span> <span data-ttu-id="19311-156">但是，如果您想要深入瞭解此模型，請閱讀原始提案：「以 [*電路為中心的量子分類器」、Maria Schuld、Alex Bocharov、Krysta Svore 和 Nathan Wiebe*](https://arxiv.org/abs/1804.00633)</span><span class="sxs-lookup"><span data-stu-id="19311-156">However, if you want to learn more about this model, please read the original proposal: [*'Circuit-centric quantum classifiers', Maria Schuld, Alex Bocharov, Krysta Svore and Nathan Wiebe*](https://arxiv.org/abs/1804.00633)</span></span>

<span data-ttu-id="19311-157">除了您在後續步驟中會看到的程式碼範例，您也可以在[本教學](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/QuantumClassification)課程中開始探索量子分類</span><span class="sxs-lookup"><span data-stu-id="19311-157">In addition to the code sample you will see in the next steps, you can also start exploring quantum classification in [this tutorial](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/QuantumClassification)</span></span> 
